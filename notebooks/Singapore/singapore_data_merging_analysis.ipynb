{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Singapore Air Quality Data Merging\n",
        "\n",
        "Merge cleaned datasets (pollutants, temperature, weather forecast) into a unified dataset for modeling.\n",
        "\n",
        "**Prerequisites:** Run `singapore_data_cleaning_processing.ipynb` first to generate:\n",
        "- `pollutants_clean.csv`\n",
        "- `airtemp_national.csv`\n",
        "- `weather_forecast_national.csv`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Load Cleaned Datasets\n",
        "\n",
        "Import libraries and load the cleaned CSV files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n",
            "Pandas version: 2.3.2\n",
            "NumPy version: 2.3.3\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base path: /Users/sharin/Downloads/COS30049/Assignment/Assignment_2/AirAware\n",
            "Data path: /Users/sharin/Downloads/COS30049/Assignment/Assignment_2/AirAware/data/singapore\n",
            "\n",
            "==================================================\n",
            "LOADING CLEANED DATASETS\n",
            "==================================================\n",
            "\n",
            "✓ Loaded pollutants data: (15980, 8)\n",
            "  Date range: 2016-02-07 00:00:00 to 2024-12-31 00:00:00\n",
            "  Regions: 5\n",
            "\n",
            "✓ Loaded temperature data: (3114, 2)\n",
            "  Date range: 2016-04-15 00:00:00 to 2024-12-31 00:00:00\n",
            "\n",
            "✓ Loaded forecast data: (3085, 2)\n",
            "  Date range: 2016-03-14 00:00:00 to 2024-12-31 00:00:00\n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Set up file paths\n",
        "base_path = Path('/Users/sharin/Downloads/COS30049/Assignment/Assignment_2/AirAware')\n",
        "data_path = base_path / 'data' / 'singapore'\n",
        "output_path = base_path / 'data' / 'singapore'\n",
        "viz_path = base_path / 'visualizations'\n",
        "\n",
        "print(f\"Base path: {base_path}\")\n",
        "print(f\"Data path: {data_path}\")\n",
        "\n",
        "# Load cleaned datasets\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"LOADING CLEANED DATASETS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Load pollutants data\n",
        "pollutants_file = data_path / 'pollutants_clean.csv'\n",
        "if pollutants_file.exists():\n",
        "    pollutants_df = pd.read_csv(pollutants_file)\n",
        "    pollutants_df['date'] = pd.to_datetime(pollutants_df['date'])\n",
        "    print(f\"\\n✓ Loaded pollutants data: {pollutants_df.shape}\")\n",
        "    print(f\"  Date range: {pollutants_df['date'].min()} to {pollutants_df['date'].max()}\")\n",
        "    print(f\"  Regions: {pollutants_df['region'].nunique()}\")\n",
        "else:\n",
        "    print(f\"\\n✗ Error: {pollutants_file} not found!\")\n",
        "    pollutants_df = None\n",
        "\n",
        "# Load temperature data\n",
        "temp_file = data_path / 'airtemp_national.csv'\n",
        "if temp_file.exists():\n",
        "    temp_df = pd.read_csv(temp_file)\n",
        "    temp_df['date'] = pd.to_datetime(temp_df['date'])\n",
        "    print(f\"\\n✓ Loaded temperature data: {temp_df.shape}\")\n",
        "    print(f\"  Date range: {temp_df['date'].min()} to {temp_df['date'].max()}\")\n",
        "else:\n",
        "    print(f\"\\n✗ Error: {temp_file} not found!\")\n",
        "    temp_df = None\n",
        "\n",
        "# Load weather forecast data\n",
        "forecast_file = data_path / 'weather_forecast_national.csv'\n",
        "if forecast_file.exists():\n",
        "    forecast_df = pd.read_csv(forecast_file)\n",
        "    forecast_df['date'] = pd.to_datetime(forecast_df['date'])\n",
        "    print(f\"\\n✓ Loaded forecast data: {forecast_df.shape}\")\n",
        "    print(f\"  Date range: {forecast_df['date'].min()} to {forecast_df['date'].max()}\")\n",
        "else:\n",
        "    print(f\"\\n✗ Error: {forecast_file} not found!\")\n",
        "    forecast_df = None\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Merge Datasets\n",
        "\n",
        "Merge pollutants, temperature, and forecast data into a single dataset.\n",
        "\n",
        "**Strategy:** Aggregate pollutants to national level, then merge all datasets by date using outer join.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== PREPARING DATASETS FOR MERGING ===\n",
            "\n",
            "Aggregating pollutants data to national level...\n",
            "  ✓ National pollutants shape: (3196, 7)\n",
            "\n",
            "==================================================\n",
            "MERGING DATASETS\n",
            "==================================================\n",
            "\n",
            "Starting with pollutants: (3196, 7)\n",
            "After merging temperature: (3229, 8)\n",
            "After merging forecast: (3229, 9)\n",
            "\n",
            "✓ Final merged dataset: (3229, 9)\n",
            "  Date range: 2016-02-07 00:00:00 to 2024-12-31 00:00:00\n",
            "  Total days: 3229\n",
            "\n",
            "First 5 rows of merged dataset:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>pm25_twenty_four_hourly</th>\n",
              "      <th>pm10_twenty_four_hourly</th>\n",
              "      <th>o3_eight_hour_max</th>\n",
              "      <th>co_eight_hour_max</th>\n",
              "      <th>so2_twenty_four_hourly</th>\n",
              "      <th>no2_one_hour_max</th>\n",
              "      <th>temperature_national</th>\n",
              "      <th>forecast_category_national</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-02-07</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>20.400000</td>\n",
              "      <td>56.600000</td>\n",
              "      <td>0.362000</td>\n",
              "      <td>3.200000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-02-08</td>\n",
              "      <td>16.800000</td>\n",
              "      <td>33.800000</td>\n",
              "      <td>42.800000</td>\n",
              "      <td>0.382000</td>\n",
              "      <td>3.800000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-02-09</td>\n",
              "      <td>18.758333</td>\n",
              "      <td>35.433333</td>\n",
              "      <td>44.341667</td>\n",
              "      <td>0.398917</td>\n",
              "      <td>3.400000</td>\n",
              "      <td>7.708333</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-02-10</td>\n",
              "      <td>16.025000</td>\n",
              "      <td>29.808333</td>\n",
              "      <td>30.950000</td>\n",
              "      <td>0.403667</td>\n",
              "      <td>3.491667</td>\n",
              "      <td>11.241667</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-02-11</td>\n",
              "      <td>8.566667</td>\n",
              "      <td>17.475000</td>\n",
              "      <td>27.700000</td>\n",
              "      <td>0.364000</td>\n",
              "      <td>3.791667</td>\n",
              "      <td>11.133333</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        date  pm25_twenty_four_hourly  pm10_twenty_four_hourly  \\\n",
              "0 2016-02-07                10.000000                20.400000   \n",
              "1 2016-02-08                16.800000                33.800000   \n",
              "2 2016-02-09                18.758333                35.433333   \n",
              "3 2016-02-10                16.025000                29.808333   \n",
              "4 2016-02-11                 8.566667                17.475000   \n",
              "\n",
              "   o3_eight_hour_max  co_eight_hour_max  so2_twenty_four_hourly  \\\n",
              "0          56.600000           0.362000                3.200000   \n",
              "1          42.800000           0.382000                3.800000   \n",
              "2          44.341667           0.398917                3.400000   \n",
              "3          30.950000           0.403667                3.491667   \n",
              "4          27.700000           0.364000                3.791667   \n",
              "\n",
              "   no2_one_hour_max  temperature_national  forecast_category_national  \n",
              "0          6.000000                   NaN                         NaN  \n",
              "1         13.000000                   NaN                         NaN  \n",
              "2          7.708333                   NaN                         NaN  \n",
              "3         11.241667                   NaN                         NaN  \n",
              "4         11.133333                   NaN                         NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Missing values:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Missing Count</th>\n",
              "      <th>Percentage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>pm25_twenty_four_hourly</th>\n",
              "      <td>33</td>\n",
              "      <td>1.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pm10_twenty_four_hourly</th>\n",
              "      <td>33</td>\n",
              "      <td>1.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>o3_eight_hour_max</th>\n",
              "      <td>33</td>\n",
              "      <td>1.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>co_eight_hour_max</th>\n",
              "      <td>41</td>\n",
              "      <td>1.27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>so2_twenty_four_hourly</th>\n",
              "      <td>33</td>\n",
              "      <td>1.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>no2_one_hour_max</th>\n",
              "      <td>33</td>\n",
              "      <td>1.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>temperature_national</th>\n",
              "      <td>115</td>\n",
              "      <td>3.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>forecast_category_national</th>\n",
              "      <td>144</td>\n",
              "      <td>4.46</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            Missing Count  Percentage\n",
              "pm25_twenty_four_hourly                33        1.02\n",
              "pm10_twenty_four_hourly                33        1.02\n",
              "o3_eight_hour_max                      33        1.02\n",
              "co_eight_hour_max                      41        1.27\n",
              "so2_twenty_four_hourly                 33        1.02\n",
              "no2_one_hour_max                       33        1.02\n",
              "temperature_national                  115        3.56\n",
              "forecast_category_national            144        4.46"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Prepare datasets for merging\n",
        "print(\"=== PREPARING DATASETS FOR MERGING ===\\n\")\n",
        "\n",
        "# Get national-level pollutants data (aggregate across regions)\n",
        "if pollutants_df is not None:\n",
        "    print(\"Aggregating pollutants data to national level...\")\n",
        "    pollutants_national = pollutants_df.groupby('date').agg({\n",
        "        'pm25_twenty_four_hourly': 'mean',\n",
        "        'pm10_twenty_four_hourly': 'mean',\n",
        "        'o3_eight_hour_max': 'mean',\n",
        "        'co_eight_hour_max': 'mean',\n",
        "        'so2_twenty_four_hourly': 'mean',\n",
        "        'no2_one_hour_max': 'mean'\n",
        "    }).reset_index()\n",
        "    print(f\"  ✓ National pollutants shape: {pollutants_national.shape}\")\n",
        "else:\n",
        "    pollutants_national = None\n",
        "\n",
        "# Merge datasets\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MERGING DATASETS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if pollutants_national is not None and temp_df is not None and forecast_df is not None:\n",
        "    # Start with pollutants data\n",
        "    merged_df = pollutants_national.copy()\n",
        "    print(f\"\\nStarting with pollutants: {merged_df.shape}\")\n",
        "    \n",
        "    # Merge temperature data\n",
        "    merged_df = pd.merge(merged_df, temp_df, on='date', how='outer')\n",
        "    print(f\"After merging temperature: {merged_df.shape}\")\n",
        "    \n",
        "    # Merge forecast data\n",
        "    merged_df = pd.merge(merged_df, forecast_df, on='date', how='outer')\n",
        "    print(f\"After merging forecast: {merged_df.shape}\")\n",
        "    \n",
        "    # Sort by date\n",
        "    merged_df = merged_df.sort_values('date').reset_index(drop=True)  # type: ignore\n",
        "    \n",
        "    print(f\"\\n✓ Final merged dataset: {merged_df.shape}\")\n",
        "    print(f\"  Date range: {merged_df['date'].min()} to {merged_df['date'].max()}\")\n",
        "    print(f\"  Total days: {len(merged_df)}\")\n",
        "    \n",
        "    # Display first few rows\n",
        "    print(f\"\\nFirst 5 rows of merged dataset:\")\n",
        "    display(merged_df.head())\n",
        "    \n",
        "    # Check for missing values\n",
        "    print(f\"\\nMissing values:\")\n",
        "    missing_summary = merged_df.isnull().sum()\n",
        "    missing_pct = (missing_summary / len(merged_df) * 100).round(2)\n",
        "    missing_df = pd.DataFrame({\n",
        "        'Missing Count': missing_summary,\n",
        "        'Percentage': missing_pct\n",
        "    })\n",
        "    display(missing_df[missing_df['Missing Count'] > 0])\n",
        "    \n",
        "else:\n",
        "    print(\"\\n✗ Error: Cannot merge datasets. One or more datasets are missing.\")\n",
        "    merged_df = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Handle Missing Values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== HANDLING MISSING VALUES ===\n",
            "\n",
            "Applying linear interpolation (limit: 3 consecutive days)...\n",
            "  pm25_twenty_four_hourly: filled 28 missing values\n",
            "  pm10_twenty_four_hourly: filled 28 missing values\n",
            "  o3_eight_hour_max: filled 28 missing values\n",
            "  co_eight_hour_max: filled 36 missing values\n",
            "  so2_twenty_four_hourly: filled 28 missing values\n",
            "  no2_one_hour_max: filled 28 missing values\n",
            "  temperature_national: filled 24 missing values\n",
            "  forecast_category_national: filled 49 missing values\n",
            "\n",
            "Dropping rows with remaining missing PM2.5 values...\n",
            "  Dropped 5 rows (0.15%)\n",
            "\n",
            "Final dataset shape: (3224, 9)\n",
            "Date range: 2016-02-07 00:00:00 to 2024-12-31 00:00:00\n",
            "\n",
            "Remaining missing values:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "temperature_national          91\n",
              "forecast_category_national    91\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Saved merged dataset to: /Users/sharin/Downloads/COS30049/Assignment/Assignment_2/AirAware/data/singapore/singapore_merged.csv\n"
          ]
        }
      ],
      "source": [
        "# Handle missing values\n",
        "if merged_df is not None:\n",
        "    print(\"=== HANDLING MISSING VALUES ===\\n\")\n",
        "    \n",
        "    # Make a copy for processing\n",
        "    processed_df = merged_df.copy()\n",
        "    \n",
        "    # For numeric columns, use linear interpolation (limited to 3 consecutive days)\n",
        "    numeric_cols = processed_df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    \n",
        "    print(\"Applying linear interpolation (limit: 3 consecutive days)...\")\n",
        "    for col in numeric_cols:\n",
        "        before_missing = processed_df[col].isnull().sum()\n",
        "        processed_df[col] = processed_df[col].interpolate(method='linear', limit=3)\n",
        "        after_missing = processed_df[col].isnull().sum()\n",
        "        filled = before_missing - after_missing\n",
        "        if filled > 0:\n",
        "            print(f\"  {col}: filled {filled} missing values\")\n",
        "    \n",
        "    # Drop rows with remaining missing values in critical columns\n",
        "    print(\"\\nDropping rows with remaining missing PM2.5 values...\")\n",
        "    before_rows = len(processed_df)\n",
        "    processed_df = processed_df.dropna(subset=['pm25_twenty_four_hourly'])\n",
        "    after_rows = len(processed_df)\n",
        "    dropped = before_rows - after_rows\n",
        "    print(f\"  Dropped {dropped} rows ({dropped/before_rows*100:.2f}%)\")\n",
        "    \n",
        "    print(f\"\\nFinal dataset shape: {processed_df.shape}\")\n",
        "    print(f\"Date range: {processed_df['date'].min()} to {processed_df['date'].max()}\")\n",
        "    \n",
        "    # Check remaining missing values\n",
        "    print(f\"\\nRemaining missing values:\")\n",
        "    missing_final = processed_df.isnull().sum()\n",
        "    if missing_final.sum() > 0:\n",
        "        display(missing_final[missing_final > 0])\n",
        "    else:\n",
        "        print(\"  ✓ No missing values!\")\n",
        "    \n",
        "    # Save merged dataset\n",
        "    output_file = output_path / 'singapore_merged.csv'\n",
        "    processed_df.to_csv(output_file, index=False)\n",
        "    print(f\"\\n✓ Saved merged dataset to: {output_file}\")\n",
        "    \n",
        "else:\n",
        "    print(\"Cannot process missing values: merged dataset not available\")\n",
        "    processed_df = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Merged dataset created: `singapore_merged.csv`\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
