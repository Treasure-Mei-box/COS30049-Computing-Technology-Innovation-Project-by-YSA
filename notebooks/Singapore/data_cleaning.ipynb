{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6d59d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "\n",
      "================================================================================\n",
      "CLEANING POLLUTANT DATA\n",
      "================================================================================\n",
      "Processing pollutant data for 2016...\n",
      "  - Processed 1914 daily records from 2016\n",
      "Processing pollutant data for 2017...\n",
      "  - Processed 2166 daily records from 2017\n",
      "Processing pollutant data for 2018...\n",
      "  - Processed 2190 daily records from 2018\n",
      "Processing pollutant data for 2019...\n",
      "  - Processed 2160 daily records from 2019\n",
      "Processing pollutant data for 2020...\n",
      "  - Processed 2172 daily records from 2020\n",
      "Processing pollutant data for 2021...\n",
      "  - Processed 2118 daily records from 2021\n",
      "Processing pollutant data for 2022...\n",
      "  - Processed 2184 daily records from 2022\n",
      "Processing pollutant data for 2023...\n",
      "  - Processed 2015 daily records from 2023\n",
      "Processing pollutant data for 2024...\n",
      "  - Processed 1830 daily records from 2024\n",
      "\n",
      "✓ Pollutant data cleaned and saved to 'pollutant_data.csv'\n",
      "  Total records: 18,749\n",
      "  Date range: 2016-02-07 to 2024-12-31\n",
      "  Regions: ['Central', 'East', 'National', 'North', 'South', 'West']\n",
      "\n",
      "  Records per region:\n",
      "    Central: 3,196 days\n",
      "    East: 3,196 days\n",
      "    National: 2,769 days\n",
      "    North: 3,196 days\n",
      "    South: 3,196 days\n",
      "    West: 3,196 days\n",
      "\n",
      "================================================================================\n",
      "POLLUTANT DATA SAMPLE\n",
      "================================================================================\n",
      "\n",
      "First 10 records:\n",
      "         Date    Region  pm25  pm10    o3   no2  so2    co   aqi\n",
      "0  2016-02-07   Central  11.0  21.0  59.0  10.0  3.0  0.45  47.0\n",
      "1  2016-02-07      East  11.0  19.0  56.0   4.0  2.0  0.41  47.0\n",
      "2  2016-02-07  National  11.0  24.0  66.0  10.0  6.0  0.45  47.0\n",
      "3  2016-02-07     North   9.0  22.0  66.0   4.0  6.0  0.25  37.0\n",
      "4  2016-02-07     South  11.0  24.0  46.0   7.0  2.0  0.45  44.0\n",
      "5  2016-02-07      West   8.0  16.0  56.0   5.0  3.0  0.25  34.0\n",
      "6  2016-02-08   Central  17.0  34.0  49.0  15.0  4.0  0.42  57.0\n",
      "7  2016-02-08      East  19.0  35.0  46.0  12.0  3.0  0.39  59.0\n",
      "8  2016-02-08  National  19.0  43.0  49.0  22.0  6.0  0.42  59.0\n",
      "9  2016-02-08     North  15.0  30.0  41.0  22.0  6.0  0.34  54.0\n",
      "\n",
      "Data types:\n",
      "Date       object\n",
      "Region     object\n",
      "pm25      float64\n",
      "pm10      float64\n",
      "o3        float64\n",
      "no2       float64\n",
      "so2       float64\n",
      "co        float64\n",
      "aqi       float64\n",
      "dtype: object\n",
      "\n",
      "Summary statistics:\n",
      "               pm25          pm10            o3          no2           so2  \\\n",
      "count  18749.000000  18749.000000  18749.000000  18749.00000  18749.000000   \n",
      "mean      13.511994     27.381782     26.225287     26.13996      8.484466   \n",
      "std        6.096358      9.220141     12.806375     11.94140      8.824163   \n",
      "min        3.090000      7.210000      2.000000      2.80000      1.000000   \n",
      "25%        9.540000     21.000000     16.880000     17.39000      3.250000   \n",
      "50%       12.390000     26.170000     24.130000     24.79000      5.330000   \n",
      "75%       16.170000     32.330000     33.290000     33.21000     10.080000   \n",
      "max       86.750000    118.330000    101.330000    117.83000    112.620000   \n",
      "\n",
      "                 co           aqi  \n",
      "count  17937.000000  18749.000000  \n",
      "mean       0.557241     47.635239  \n",
      "std        0.210617     12.031316  \n",
      "min        0.110000     14.290000  \n",
      "25%        0.420000     39.210000  \n",
      "50%        0.520000     49.710000  \n",
      "75%        0.660000     55.480000  \n",
      "max        3.480000    133.380000  \n",
      "\n",
      "Missing values:\n",
      "    Missing Count  Percentage\n",
      "co            812        4.33\n",
      "\n",
      "================================================================================\n",
      "CLEANING WEATHER DATA\n",
      "================================================================================\n",
      "\n",
      "Processing weather data for 2016...\n",
      "  Reading temperature data...\n",
      "    - 1110 daily temperature records processed\n",
      "  Reading humidity data...\n",
      "    - 935 daily humidity records processed\n",
      "  Reading wind speed data...\n",
      "    - 120 daily wind speed records processed\n",
      "\n",
      "✓ Weather data for 2016 saved to 'weather_2016.csv'\n",
      "  Total records: 1,140\n",
      "  Date range: 2016-04-15 to 2016-12-31\n",
      "  Regions: ['Central', 'East', 'North', 'South', 'West']\n",
      "\n",
      "Processing weather data for 2017...\n",
      "  Reading temperature data...\n",
      "    - 1797 daily temperature records processed\n",
      "  Reading humidity data...\n",
      "    - 1797 daily humidity records processed\n",
      "  Reading wind speed data...\n",
      "    - 1685 daily wind speed records processed\n",
      "\n",
      "✓ Weather data for 2017 saved to 'weather_2017.csv'\n",
      "  Total records: 1,797\n",
      "  Date range: 2017-01-01 to 2017-12-31\n",
      "  Regions: ['Central', 'East', 'North', 'South', 'West']\n",
      "\n",
      "Processing weather data for 2018...\n",
      "  Reading temperature data...\n",
      "    - 1793 daily temperature records processed\n",
      "  Reading humidity data...\n",
      "    - 1792 daily humidity records processed\n",
      "  Reading wind speed data...\n",
      "    - 1772 daily wind speed records processed\n",
      "\n",
      "✓ Weather data for 2018 saved to 'weather_2018.csv'\n",
      "  Total records: 1,813\n",
      "  Date range: 2018-01-01 to 2018-12-31\n",
      "  Regions: ['Central', 'East', 'North', 'South', 'West']\n",
      "\n",
      "Processing weather data for 2019...\n",
      "  Reading temperature data...\n",
      "    - 1765 daily temperature records processed\n",
      "  Reading humidity data...\n",
      "    - 1650 daily humidity records processed\n",
      "  Reading wind speed data...\n",
      "    - 1660 daily wind speed records processed\n",
      "\n",
      "✓ Weather data for 2019 saved to 'weather_2019.csv'\n",
      "  Total records: 1,790\n",
      "  Date range: 2019-01-01 to 2019-12-31\n",
      "  Regions: ['Central', 'East', 'North', 'South', 'West']\n",
      "\n",
      "Processing weather data for 2020...\n",
      "  Reading temperature data...\n",
      "    - 1818 daily temperature records processed\n",
      "  Reading humidity data...\n",
      "    - 1818 daily humidity records processed\n",
      "  Reading wind speed data...\n",
      "    - 1813 daily wind speed records processed\n",
      "\n",
      "✓ Weather data for 2020 saved to 'weather_2020.csv'\n",
      "  Total records: 1,828\n",
      "  Date range: 2020-01-01 to 2020-12-31\n",
      "  Regions: ['Central', 'East', 'North', 'South', 'West']\n",
      "\n",
      "Processing weather data for 2021...\n",
      "  Reading temperature data...\n",
      "    - 1810 daily temperature records processed\n",
      "  Reading humidity data...\n",
      "    - 1809 daily humidity records processed\n",
      "  Reading wind speed data...\n",
      "    - 1785 daily wind speed records processed\n",
      "\n",
      "✓ Weather data for 2021 saved to 'weather_2021.csv'\n",
      "  Total records: 1,810\n",
      "  Date range: 2021-01-01 to 2021-12-31\n",
      "  Regions: ['Central', 'East', 'North', 'South', 'West']\n",
      "\n",
      "Processing weather data for 2022...\n",
      "  Reading temperature data...\n",
      "    - 1825 daily temperature records processed\n",
      "  Reading humidity data...\n",
      "    - 1825 daily humidity records processed\n",
      "  Reading wind speed data...\n",
      "    - 1817 daily wind speed records processed\n",
      "\n",
      "✓ Weather data for 2022 saved to 'weather_2022.csv'\n",
      "  Total records: 1,825\n",
      "  Date range: 2022-01-01 to 2022-12-31\n",
      "  Regions: ['Central', 'East', 'North', 'South', 'West']\n",
      "\n",
      "Processing weather data for 2023...\n",
      "  Reading temperature data...\n",
      "    - 1780 daily temperature records processed\n",
      "  Reading humidity data...\n",
      "    - 1780 daily humidity records processed\n",
      "  Reading wind speed data...\n",
      "    - 1755 daily wind speed records processed\n",
      "\n",
      "✓ Weather data for 2023 saved to 'weather_2023.csv'\n",
      "  Total records: 1,780\n",
      "  Date range: 2023-01-01 to 2023-12-31\n",
      "  Regions: ['Central', 'East', 'North', 'South', 'West']\n",
      "\n",
      "Processing weather data for 2024...\n",
      "  Reading temperature data...\n",
      "    - 1802 daily temperature records processed\n",
      "  Reading humidity data...\n",
      "    - 1802 daily humidity records processed\n",
      "  Reading wind speed data...\n",
      "    - 1776 daily wind speed records processed\n",
      "\n",
      "✓ Weather data for 2024 saved to 'weather_2024.csv'\n",
      "  Total records: 1,804\n",
      "  Date range: 2024-01-01 to 2024-12-31\n",
      "  Regions: ['Central', 'East', 'North', 'South', 'West']\n",
      "\n",
      "================================================================================\n",
      "WEATHER DATA SAMPLE (2024)\n",
      "================================================================================\n",
      "\n",
      "First 10 records:\n",
      "     Country   Region        Date  Temperature  RelativeHumidity  WindSpeed\n",
      "0  Singapore  Central  2024-01-01        25.98             93.18       6.04\n",
      "1  Singapore     East  2024-01-01        26.02             90.26       5.23\n",
      "2  Singapore    North  2024-01-01        25.56             91.71       3.18\n",
      "3  Singapore    South  2024-01-01        26.38             87.11       2.33\n",
      "4  Singapore     West  2024-01-01        26.13             91.86       3.47\n",
      "5  Singapore  Central  2024-01-02        27.50             81.71       6.17\n",
      "6  Singapore     East  2024-01-02        27.75             79.64       5.84\n",
      "7  Singapore    North  2024-01-02        27.14             81.52       4.22\n",
      "8  Singapore    South  2024-01-02        25.10             90.65       1.11\n",
      "9  Singapore     West  2024-01-02        27.53             82.79       3.60\n",
      "\n",
      "Data types:\n",
      "Country              object\n",
      "Region               object\n",
      "Date                 object\n",
      "Temperature         float64\n",
      "RelativeHumidity    float64\n",
      "WindSpeed           float64\n",
      "dtype: object\n",
      "\n",
      "Summary statistics:\n",
      "       Temperature  RelativeHumidity    WindSpeed\n",
      "count  1802.000000       1802.000000  1776.000000\n",
      "mean     28.483607         77.407137     4.258733\n",
      "std       1.158008          5.875984     1.563515\n",
      "min      24.240000         60.970000     1.110000\n",
      "25%      27.710000         73.255000     3.000000\n",
      "50%      28.550000         76.780000     4.040000\n",
      "75%      29.370000         81.107500     5.210000\n",
      "max      31.690000         97.260000    20.700000\n",
      "\n",
      "Missing values:\n",
      "                  Missing Count  Percentage\n",
      "Temperature                   2        0.11\n",
      "RelativeHumidity              2        0.11\n",
      "WindSpeed                    28        1.55\n",
      "\n",
      "================================================================================\n",
      "DATA CLEANING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "📊 POLLUTANT DATA:\n",
      "  ✓ File: pollutant_data.csv\n",
      "  ✓ Records: 18,749\n",
      "  ✓ Columns: ['Date', 'Region', 'pm25', 'pm10', 'o3', 'no2', 'so2', 'co', 'aqi']\n",
      "  ✓ Date range: 2016-02-07 to 2024-12-31\n",
      "  ✓ Regions: ['Central', 'East', 'National', 'North', 'South', 'West']\n",
      "  ✓ Missing values:\n",
      "      co: 812 (4.33%)\n",
      "\n",
      "🌤️  WEATHER DATA:\n",
      "  ✓ weather_2016.csv:\n",
      "      Records: 1,140\n",
      "      Columns: ['Country', 'Region', 'Date', 'Temperature', 'RelativeHumidity', 'WindSpeed']\n",
      "      Date range: 2016-04-15 to 2016-12-31\n",
      "      Missing: Temperature=30, RelativeHumidity=205, WindSpeed=1020\n",
      "  ✓ weather_2017.csv:\n",
      "      Records: 1,797\n",
      "      Columns: ['Country', 'Region', 'Date', 'Temperature', 'RelativeHumidity', 'WindSpeed']\n",
      "      Date range: 2017-01-01 to 2017-12-31\n",
      "      Missing: WindSpeed=112\n",
      "  ✓ weather_2018.csv:\n",
      "      Records: 1,813\n",
      "      Columns: ['Country', 'Region', 'Date', 'Temperature', 'RelativeHumidity', 'WindSpeed']\n",
      "      Date range: 2018-01-01 to 2018-12-31\n",
      "      Missing: Temperature=20, RelativeHumidity=21, WindSpeed=41\n",
      "  ✓ weather_2019.csv:\n",
      "      Records: 1,790\n",
      "      Columns: ['Country', 'Region', 'Date', 'Temperature', 'RelativeHumidity', 'WindSpeed']\n",
      "      Date range: 2019-01-01 to 2019-12-31\n",
      "      Missing: Temperature=25, RelativeHumidity=140, WindSpeed=130\n",
      "  ✓ weather_2020.csv:\n",
      "      Records: 1,828\n",
      "      Columns: ['Country', 'Region', 'Date', 'Temperature', 'RelativeHumidity', 'WindSpeed']\n",
      "      Date range: 2020-01-01 to 2020-12-31\n",
      "      Missing: Temperature=10, RelativeHumidity=10, WindSpeed=15\n",
      "  ✓ weather_2021.csv:\n",
      "      Records: 1,810\n",
      "      Columns: ['Country', 'Region', 'Date', 'Temperature', 'RelativeHumidity', 'WindSpeed']\n",
      "      Date range: 2021-01-01 to 2021-12-31\n",
      "      Missing: RelativeHumidity=1, WindSpeed=25\n",
      "  ✓ weather_2022.csv:\n",
      "      Records: 1,825\n",
      "      Columns: ['Country', 'Region', 'Date', 'Temperature', 'RelativeHumidity', 'WindSpeed']\n",
      "      Date range: 2022-01-01 to 2022-12-31\n",
      "      Missing: WindSpeed=8\n",
      "  ✓ weather_2023.csv:\n",
      "      Records: 1,780\n",
      "      Columns: ['Country', 'Region', 'Date', 'Temperature', 'RelativeHumidity', 'WindSpeed']\n",
      "      Date range: 2023-01-01 to 2023-12-31\n",
      "      Missing: WindSpeed=25\n",
      "  ✓ weather_2024.csv:\n",
      "      Records: 1,804\n",
      "      Columns: ['Country', 'Region', 'Date', 'Temperature', 'RelativeHumidity', 'WindSpeed']\n",
      "      Date range: 2024-01-01 to 2024-12-31\n",
      "      Missing: Temperature=2, RelativeHumidity=2, WindSpeed=28\n",
      "\n",
      "================================================================================\n",
      "✓ Data cleaning completed successfully!\n",
      "================================================================================\n",
      "\n",
      "📋 OUTPUT FILES:\n",
      "  - pollutant_data.csv (single file, 2016-2024, daily aggregation)\n",
      "  - weather_2016.csv through weather_2024.csv (yearly files, daily aggregation)\n",
      "\n",
      "💡 Both datasets now have ONE RECORD PER REGION PER DAY\n"
     ]
    }
   ],
   "source": [
    "# Singapore Air Quality and Weather Data Cleaning\n",
    "# This notebook cleans and processes:\n",
    "# 1. Pollutant data (2016-2024) -> single output file with DAILY aggregation\n",
    "# 2. Weather data (air temperature, humidity, wind speed) -> yearly output files with DAILY aggregation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. CLEAN POLLUTANT DATA - DAILY AGGREGATION\n",
    "# ============================================================================\n",
    "\n",
    "def clean_pollutant_data(years=range(2016, 2025), aggregate='day'):\n",
    "    \"\"\"\n",
    "    Clean and combine pollutant data from all years\n",
    "    Output columns: Region, Date, pm25, pm10, o3, no2, so2, co, aqi\n",
    "\n",
    "    Parameters:\n",
    "      - years: iterable of years to process\n",
    "      - aggregate: 'none' | 'hour' | 'day' -> how to aggregate timestamps\n",
    "        'day' (recommended): One record per region per day with daily averages\n",
    "        'hour': One record per region per hour with hourly averages\n",
    "        'none': Keep all raw timestamps (may have multiple per day)\n",
    "    \"\"\"\n",
    "    all_pollutant_data = []\n",
    "    \n",
    "    for year in years:\n",
    "        file_path = f'pollutant/pollutants_{year}.csv'\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Warning: {file_path} not found, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing pollutant data for {year}...\")\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Parse datetime\n",
    "        df['Date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "        \n",
    "        # Build cleaned dataframe with numeric conversions\n",
    "        df_clean = pd.DataFrame({\n",
    "            'Region': df['region'].str.title(),\n",
    "            'Date': df['Date'],\n",
    "            'pm25': pd.to_numeric(df['pm25_twenty_four_hourly'], errors='coerce'),\n",
    "            'pm10': pd.to_numeric(df['pm10_twenty_four_hourly'], errors='coerce'),\n",
    "            'o3': pd.to_numeric(df['o3_eight_hour_max'], errors='coerce'),\n",
    "            'no2': pd.to_numeric(df['no2_one_hour_max'], errors='coerce'),\n",
    "            'so2': pd.to_numeric(df['so2_twenty_four_hourly'], errors='coerce'),\n",
    "            'co': pd.to_numeric(df['co_eight_hour_max'], errors='coerce'),\n",
    "            'aqi': pd.to_numeric(df['psi_twenty_four_hourly'], errors='coerce')\n",
    "        })\n",
    "        \n",
    "        # Remove rows with all missing values in numeric columns\n",
    "        numeric_columns = ['pm25', 'pm10', 'o3', 'no2', 'so2', 'co', 'aqi']\n",
    "        df_clean = df_clean.dropna(subset=numeric_columns, how='all')\n",
    "        \n",
    "        # Apply aggregation based on parameter\n",
    "        if aggregate == 'day':\n",
    "            # Floor to day level (remove time component)\n",
    "            df_clean['Date'] = df_clean['Date'].dt.floor('D')\n",
    "            # Group by Date (day) and Region, take mean of all numeric columns\n",
    "            grouped = df_clean.groupby(['Date', 'Region'])[numeric_columns].mean().reset_index()\n",
    "            # Round numeric columns to 2 decimal places\n",
    "            for col in numeric_columns:\n",
    "                grouped[col] = grouped[col].round(2)\n",
    "            # Format date as YYYY-MM-DD (no time component needed for daily data)\n",
    "            grouped['Date'] = grouped['Date'].dt.strftime('%Y-%m-%d')\n",
    "            all_pollutant_data.append(grouped)\n",
    "            print(f\"  - Processed {len(grouped)} daily records from {year}\")\n",
    "            \n",
    "        elif aggregate == 'hour':\n",
    "            # Floor to hour level\n",
    "            df_clean['Date'] = df_clean['Date'].dt.floor('H')\n",
    "            # Group by Date (hour) and Region\n",
    "            grouped = df_clean.groupby(['Date', 'Region'])[numeric_columns].mean().reset_index()\n",
    "            # Round numeric columns\n",
    "            for col in numeric_columns:\n",
    "                grouped[col] = grouped[col].round(2)\n",
    "            # Format with hour precision\n",
    "            grouped['Date'] = grouped['Date'].dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "            all_pollutant_data.append(grouped)\n",
    "            print(f\"  - Processed {len(grouped)} hourly records from {year}\")\n",
    "            \n",
    "        else:  # aggregate == 'none'\n",
    "            # Keep raw timestamps, just format them\n",
    "            df_clean['Date'] = df_clean['Date'].dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "            # Round numeric columns\n",
    "            for col in numeric_columns:\n",
    "                df_clean[col] = df_clean[col].round(2)\n",
    "            all_pollutant_data.append(df_clean)\n",
    "            print(f\"  - Processed {len(df_clean)} raw records from {year}\")\n",
    "    \n",
    "    # Combine all years\n",
    "    if all_pollutant_data:\n",
    "        combined_df = pd.concat(all_pollutant_data, ignore_index=True)\n",
    "        \n",
    "        # Sort by datetime and region\n",
    "        combined_df = combined_df.sort_values(['Date', 'Region']).reset_index(drop=True)\n",
    "        \n",
    "        # Save to CSV\n",
    "        output_file = 'pollutant_data.csv'\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(f\"\\n✓ Pollutant data cleaned and saved to '{output_file}'\")\n",
    "        print(f\"  Total records: {len(combined_df):,}\")\n",
    "        print(f\"  Date range: {combined_df['Date'].min()} to {combined_df['Date'].max()}\")\n",
    "        print(f\"  Regions: {sorted(combined_df['Region'].unique())}\")\n",
    "        \n",
    "        # Show records per region to verify daily aggregation\n",
    "        if aggregate == 'day':\n",
    "            print(f\"\\n  Records per region:\")\n",
    "            records_per_region = combined_df.groupby('Region').size()\n",
    "            for region, count in records_per_region.items():\n",
    "                print(f\"    {region}: {count:,} days\")\n",
    "        \n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No pollutant data files found!\")\n",
    "        return None\n",
    "\n",
    "# Execute pollutant data cleaning with DAILY aggregation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLEANING POLLUTANT DATA\")\n",
    "print(\"=\"*80)\n",
    "pollutant_df = clean_pollutant_data(aggregate='day')\n",
    "\n",
    "# Display sample of cleaned pollutant data\n",
    "if pollutant_df is not None:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"POLLUTANT DATA SAMPLE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nFirst 10 records:\")\n",
    "    print(pollutant_df.head(10))\n",
    "    print(\"\\nData types:\")\n",
    "    print(pollutant_df.dtypes)\n",
    "    print(\"\\nSummary statistics:\")\n",
    "    print(pollutant_df.describe())\n",
    "    print(\"\\nMissing values:\")\n",
    "    missing = pollutant_df.isnull().sum()\n",
    "    missing_pct = (missing / len(pollutant_df) * 100).round(2)\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing,\n",
    "        'Percentage': missing_pct\n",
    "    })\n",
    "    print(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "# ============================================================================\n",
    "# 2. CLEAN WEATHER DATA - DAILY AGGREGATION\n",
    "# ============================================================================\n",
    "\n",
    "def map_station_to_region(station_name):\n",
    "    \"\"\"\n",
    "    Map station names to regions based on Singapore geography\n",
    "    \"\"\"\n",
    "    station_name = station_name.lower()\n",
    "    \n",
    "    # North region stations\n",
    "    north_keywords = ['ang mo kio', 'sembawang', 'yishun', 'woodlands', 'admiralty', \n",
    "                      'seletar', 'pulau ubin', 'mandai']\n",
    "    # South region stations\n",
    "    south_keywords = ['sentosa', 'marina', 'tanjong', 'harbourfront', 'alexandra',\n",
    "                      'bukit merah', 'telok blangah', 'keppel']\n",
    "    # East region stations\n",
    "    east_keywords = ['changi', 'pasir ris', 'tampines', 'bedok', 'east coast',\n",
    "                     'simei', 'paya lebar', 'kim chuan']\n",
    "    # West region stations\n",
    "    west_keywords = ['jurong', 'boon lay', 'tuas', 'choa chu kang', 'bukit batok',\n",
    "                     'bukit panjang', 'clementi', 'banyan', 'nanyang']\n",
    "    # Central region stations\n",
    "    central_keywords = ['newton', 'bishan', 'serangoon', 'toa payoh', 'novena',\n",
    "                        'bukit timah', 'orchard', 'tai seng']\n",
    "    \n",
    "    for keyword in north_keywords:\n",
    "        if keyword in station_name:\n",
    "            return 'North'\n",
    "    for keyword in south_keywords:\n",
    "        if keyword in station_name:\n",
    "            return 'South'\n",
    "    for keyword in east_keywords:\n",
    "        if keyword in station_name:\n",
    "            return 'East'\n",
    "    for keyword in west_keywords:\n",
    "        if keyword in station_name:\n",
    "            return 'West'\n",
    "    for keyword in central_keywords:\n",
    "        if keyword in station_name:\n",
    "            return 'Central'\n",
    "    \n",
    "    # Default to Central if no match\n",
    "    return 'Central'\n",
    "\n",
    "def clean_weather_data_by_year(year):\n",
    "    \"\"\"\n",
    "    Clean and combine weather data (temperature, humidity, wind speed) for a specific year\n",
    "    Output columns: Country, Region, Date, Temperature, RelativeHumidity, WindSpeed\n",
    "    Daily aggregation: One record per region per day\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing weather data for {year}...\")\n",
    "    \n",
    "    # File paths\n",
    "    temp_file = f'weather/air_temperature/air_temperature_{year}.csv'\n",
    "    humidity_file = f'weather/humidity/humidity_{year}.csv'\n",
    "    wind_file = f'weather/wind_speed/wind_speed_{year}.csv'\n",
    "    \n",
    "    dataframes = {}\n",
    "    \n",
    "    # Read air temperature data\n",
    "    if os.path.exists(temp_file):\n",
    "        print(f\"  Reading temperature data...\")\n",
    "        df_temp = pd.read_csv(temp_file)\n",
    "        # Parse datetime and floor to day\n",
    "        df_temp['Date'] = pd.to_datetime(df_temp['date'], errors='coerce').dt.floor('D')\n",
    "        df_temp['Region'] = df_temp['station_name'].apply(map_station_to_region)\n",
    "        \n",
    "        # Group by Date (day) and Region, take mean\n",
    "        df_temp = df_temp.groupby(['Date', 'Region']).agg({\n",
    "            'reading_value': 'mean'\n",
    "        }).reset_index()\n",
    "        df_temp.columns = ['Date', 'Region', 'Temperature']\n",
    "        df_temp['Temperature'] = df_temp['Temperature'].round(2)\n",
    "        dataframes['temp'] = df_temp\n",
    "        print(f\"    - {len(df_temp)} daily temperature records processed\")\n",
    "    else:\n",
    "        print(f\"  Warning: {temp_file} not found\")\n",
    "    \n",
    "    # Read humidity data\n",
    "    if os.path.exists(humidity_file):\n",
    "        print(f\"  Reading humidity data...\")\n",
    "        df_humidity = pd.read_csv(humidity_file)\n",
    "        df_humidity['Date'] = pd.to_datetime(df_humidity['date'], errors='coerce').dt.floor('D')\n",
    "        df_humidity['Region'] = df_humidity['station_name'].apply(map_station_to_region)\n",
    "\n",
    "        # Group by Date (day) and Region, take mean\n",
    "        df_humidity = df_humidity.groupby(['Date', 'Region']).agg({\n",
    "            'reading_value': 'mean'\n",
    "        }).reset_index()\n",
    "        df_humidity.columns = ['Date', 'Region', 'RelativeHumidity']\n",
    "        df_humidity['RelativeHumidity'] = df_humidity['RelativeHumidity'].round(2)\n",
    "        dataframes['humidity'] = df_humidity\n",
    "        print(f\"    - {len(df_humidity)} daily humidity records processed\")\n",
    "    else:\n",
    "        print(f\"  Warning: {humidity_file} not found\")\n",
    "    \n",
    "    # Read wind speed data\n",
    "    if os.path.exists(wind_file):\n",
    "        print(f\"  Reading wind speed data...\")\n",
    "        df_wind = pd.read_csv(wind_file)\n",
    "        df_wind['Date'] = pd.to_datetime(df_wind['date'], errors='coerce').dt.floor('D')\n",
    "        df_wind['Region'] = df_wind['station_name'].apply(map_station_to_region)\n",
    "\n",
    "        # Group by Date (day) and Region, take mean\n",
    "        df_wind = df_wind.groupby(['Date', 'Region']).agg({\n",
    "            'reading_value': 'mean'\n",
    "        }).reset_index()\n",
    "        df_wind.columns = ['Date', 'Region', 'WindSpeed']\n",
    "        df_wind['WindSpeed'] = df_wind['WindSpeed'].round(2)\n",
    "        dataframes['wind'] = df_wind\n",
    "        print(f\"    - {len(df_wind)} daily wind speed records processed\")\n",
    "    else:\n",
    "        print(f\"  Warning: {wind_file} not found\")\n",
    "    \n",
    "    # Merge all weather data\n",
    "    if dataframes:\n",
    "        # Start with first available dataframe\n",
    "        df_combined = list(dataframes.values())[0].copy()\n",
    "        \n",
    "        # Merge with remaining dataframes\n",
    "        for key, df in list(dataframes.items())[1:]:\n",
    "            df_combined = pd.merge(df_combined, df, on=['Date', 'Region'], how='outer')\n",
    "        \n",
    "        # Add Country column\n",
    "        df_combined.insert(0, 'Country', 'Singapore')\n",
    "        \n",
    "        # Format date as YYYY-MM-DD\n",
    "        df_combined['Date'] = df_combined['Date'].dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Reorder columns\n",
    "        columns_order = ['Country', 'Region', 'Date', 'Temperature', 'RelativeHumidity', 'WindSpeed']\n",
    "        columns_order = [col for col in columns_order if col in df_combined.columns]\n",
    "        df_combined = df_combined[columns_order]\n",
    "        \n",
    "        # Sort by date and region\n",
    "        df_combined = df_combined.sort_values(['Date', 'Region']).reset_index(drop=True)\n",
    "        \n",
    "        # Round all numeric columns to 2 decimal places\n",
    "        numeric_cols = df_combined.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            df_combined[col] = df_combined[col].round(2)\n",
    "        \n",
    "        # Save to CSV\n",
    "        output_file = f'weather_{year}.csv'\n",
    "        df_combined.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(f\"\\n✓ Weather data for {year} saved to '{output_file}'\")\n",
    "        print(f\"  Total records: {len(df_combined):,}\")\n",
    "        print(f\"  Date range: {df_combined['Date'].min()} to {df_combined['Date'].max()}\")\n",
    "        print(f\"  Regions: {sorted(df_combined['Region'].unique())}\")\n",
    "        \n",
    "        return df_combined\n",
    "    else:\n",
    "        print(f\"No weather data files found for {year}!\")\n",
    "        return None\n",
    "\n",
    "# Process weather data for all years\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLEANING WEATHER DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "weather_dataframes = {}\n",
    "for year in range(2016, 2025):\n",
    "    df = clean_weather_data_by_year(year)\n",
    "    if df is not None:\n",
    "        weather_dataframes[year] = df\n",
    "\n",
    "# Display sample of cleaned weather data (latest year)\n",
    "if weather_dataframes:\n",
    "    latest_year = max(weather_dataframes.keys())\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"WEATHER DATA SAMPLE ({latest_year})\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nFirst 10 records:\")\n",
    "    print(weather_dataframes[latest_year].head(10))\n",
    "    print(\"\\nData types:\")\n",
    "    print(weather_dataframes[latest_year].dtypes)\n",
    "    print(\"\\nSummary statistics:\")\n",
    "    print(weather_dataframes[latest_year].describe())\n",
    "    print(\"\\nMissing values:\")\n",
    "    missing = weather_dataframes[latest_year].isnull().sum()\n",
    "    missing_pct = (missing / len(weather_dataframes[latest_year]) * 100).round(2)\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing,\n",
    "        'Percentage': missing_pct\n",
    "    })\n",
    "    print(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "# ============================================================================\n",
    "# 3. SUMMARY AND VERIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA CLEANING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n📊 POLLUTANT DATA:\")\n",
    "if os.path.exists('pollutant_data.csv'):\n",
    "    df_check = pd.read_csv('pollutant_data.csv')\n",
    "    print(f\"  ✓ File: pollutant_data.csv\")\n",
    "    print(f\"  ✓ Records: {len(df_check):,}\")\n",
    "    print(f\"  ✓ Columns: {list(df_check.columns)}\")\n",
    "    print(f\"  ✓ Date range: {df_check['Date'].min()} to {df_check['Date'].max()}\")\n",
    "    print(f\"  ✓ Regions: {sorted(df_check['Region'].unique())}\")\n",
    "    print(f\"  ✓ Missing values:\")\n",
    "    for col in df_check.columns:\n",
    "        missing = df_check[col].isna().sum()\n",
    "        if missing > 0:\n",
    "            print(f\"      {col}: {missing:,} ({missing/len(df_check)*100:.2f}%)\")\n",
    "else:\n",
    "    print(\"  ✗ File not found\")\n",
    "\n",
    "print(\"\\n🌤️  WEATHER DATA:\")\n",
    "for year in range(2016, 2025):\n",
    "    filename = f'weather_{year}.csv'\n",
    "    if os.path.exists(filename):\n",
    "        df_check = pd.read_csv(filename)\n",
    "        print(f\"  ✓ {filename}:\")\n",
    "        print(f\"      Records: {len(df_check):,}\")\n",
    "        print(f\"      Columns: {list(df_check.columns)}\")\n",
    "        print(f\"      Date range: {df_check['Date'].min()} to {df_check['Date'].max()}\")\n",
    "        missing_summary = []\n",
    "        for col in df_check.select_dtypes(include=[np.number]).columns:\n",
    "            missing = df_check[col].isna().sum()\n",
    "            if missing > 0:\n",
    "                missing_summary.append(f\"{col}={missing}\")\n",
    "        if missing_summary:\n",
    "            print(f\"      Missing: {', '.join(missing_summary)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Data cleaning completed successfully!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n📋 OUTPUT FILES:\")\n",
    "print(\"  - pollutant_data.csv (single file, 2016-2024, daily aggregation)\")\n",
    "print(\"  - weather_2016.csv through weather_2024.csv (yearly files, daily aggregation)\")\n",
    "print(\"\\n💡 Both datasets now have ONE RECORD PER REGION PER DAY\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
